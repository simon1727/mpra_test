{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpra import MPRA_Collection, MPRA_Paper, MPRA_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature_2022_Regev\n",
      "GenomeResearch_2017_Seelig\n",
      "Nature_2022_Regev: ['train_complex', 'train_defined', 'test_complex', 'test_defined']\n",
      "GenomeResearch_2017_Seelig: ['random', 'native']\n"
     ]
    }
   ],
   "source": [
    "folder_mpra = '/data/tuxm/project/MPRA-collection/data/mpra_test/'\n",
    "\n",
    "mpra_collection = MPRA_Collection(folder_mpra)\n",
    "print('\\n'.join(mpra_collection.list_papers()))\n",
    "print('\\n'.join([f'{k}: {v}' for k, v in mpra_collection.list_datasets().items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ==== ==== ====\n",
      "Description: Random 5' UTRs\n",
      "MPRA Technique: Classic MPRA\n",
      "Readout Assay: RNA-seq\n",
      "Regulatory Element: 5' UTR\n",
      "Sequence Origin: Random\n",
      "Species: Yeast\n",
      "==== ==== ==== ====\n",
      "len(mpra_dataset): 489348\n",
      "mpra_dataset.data.shape: (489348, 4)\n",
      "mpra_dataset._X.shape: torch.Size([489348, 4, 64])\n",
      "mpra_dataset._Y.shape: (489348, 1)\n",
      "mpra_dataset.X.shape: torch.Size([489348, 4, 64])\n",
      "mpra_dataset.Y.shape: torch.Size([489348, 1])\n"
     ]
    }
   ],
   "source": [
    "mpra_dataset = mpra_collection.get_dataset('GenomeResearch_2017_Seelig', 'random')\n",
    "\n",
    "mpra_dataset.load()\n",
    "# mpra_dataset now contains:\n",
    "## info: dict()\n",
    "## data: pd.DataFrame()\n",
    "mpra_dataset.print_info()\n",
    "print('len(mpra_dataset):', len(mpra_dataset))\n",
    "print('mpra_dataset.data.shape:', mpra_dataset.data.shape)\n",
    "\n",
    "mpra_dataset.init_XYobs()\n",
    "# mpra_dataset now contains:\n",
    "## _X: torch.Tensor()\n",
    "## _Y: pd.DataFrame()\n",
    "## X: torch.Tensor()\n",
    "## Y: torch.Tensor()\n",
    "print('mpra_dataset._X.shape:', mpra_dataset._X.shape)\n",
    "print('mpra_dataset._Y.shape:', mpra_dataset._Y.shape)\n",
    "print('mpra_dataset.X.shape:', mpra_dataset.X.shape)\n",
    "print('mpra_dataset.Y.shape:', mpra_dataset.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_train[0].shape:  torch.Size([391480, 4, 64])\n",
      "split_valid[1].shape:  torch.Size([48934, 1])\n"
     ]
    }
   ],
   "source": [
    "splits = mpra_dataset.split_rand(fracs=[0.8, 0.1, 0.1], seed=20240404)\n",
    "split_train, split_valid, split_infer = splits\n",
    "# each split now contains: 0-X, 1-Y, 2-obsX, 3-obsY\n",
    "print('split_train[0].shape: ', split_train[0].shape)\n",
    "print('split_valid[1].shape: ', split_valid[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_workers = 4\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataloader_train = DataLoader(TensorDataset(split_train[0], split_train[1]), batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "dataloader_valid = DataLoader(TensorDataset(split_valid[0], split_valid[1]), batch_size=batch_size, num_workers=n_workers, shuffle=False)\n",
    "dataloader_infer = DataLoader(TensorDataset(split_infer[0], split_infer[1]), batch_size=batch_size, num_workers=n_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightningModule\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "def ConvBlock(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size,\n",
    "    padding = 'same',\n",
    "    stride = 1,\n",
    "    dilation = 1,\n",
    "    bias = True,\n",
    "    batchnorm = True,\n",
    "    activation = 'relu',\n",
    "):\n",
    "    layers = [nn.BatchNorm1d(in_channels)] if batchnorm else []\n",
    "    layers.append(nn.Conv1d(\n",
    "        in_channels = in_channels,\n",
    "        out_channels = out_channels,\n",
    "        kernel_size = kernel_size,\n",
    "        padding = padding,\n",
    "        stride = stride,\n",
    "        dilation = dilation,\n",
    "        bias = bias,\n",
    "    ))\n",
    "\n",
    "    if activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'leaky_relu':\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    elif activation == 'gelu':\n",
    "        layers.append(nn.GELU())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# from https://github.com/boxiangliu/enformer-pytorch/blob/main/model/enformer.py\n",
    "class Residual(nn.Module):\n",
    "    # makes the module residual\n",
    "\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self._module = module\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self._module(x, *args, **kwargs)\n",
    "\n",
    "class ResidualCNN(LightningModule):\n",
    "    def __init__(self,\n",
    "        len_seq,\n",
    "        channels_conv = 256,\n",
    "        channels_linear = 256,\n",
    "        names_readout = [''],\n",
    "        channels_out = 1,\n",
    "        # channels_z = 0,\n",
    "        channels_z = 1,\n",
    "        kernel_size = 3,\n",
    "        kernel_size_in = 13,\n",
    "        n_layers_conv = 3,\n",
    "        n_convs_per_layer = 3,\n",
    "        n_layers_linear = 3,\n",
    "        padding = 1,\n",
    "        stride = 1,\n",
    "        dilation = 1,\n",
    "        bias = True,\n",
    "        batchnorm = False,\n",
    "        activation = 'relu',\n",
    "        pooling_type = 'max',\n",
    "        pooling_size = 4,\n",
    "        dropout = 0.1,\n",
    "        loss_type = 'mse',\n",
    "        optimizer_type = 'Adam',\n",
    "        lr = 1e-4,\n",
    "        wd = 1e-5,\n",
    "        scheduler_type = 'Cyclic',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.names_readout = names_readout\n",
    "        self.channels_z = channels_z\n",
    "\n",
    "        self.conv_in = ConvBlock(\n",
    "            in_channels = 4,\n",
    "            out_channels = channels_conv,\n",
    "            kernel_size = kernel_size_in,\n",
    "            padding = (kernel_size_in - 1) // 2,\n",
    "            stride = 1,\n",
    "            dilation = 1,\n",
    "            bias = bias,\n",
    "            batchnorm = batchnorm,\n",
    "            activation = activation,\n",
    "        )\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        channels_len = len_seq\n",
    "        for _ in range(n_layers_conv):\n",
    "            for _ in range(n_convs_per_layer):\n",
    "                self.convs.append(\n",
    "                    Residual(\n",
    "                        ConvBlock(\n",
    "                            in_channels = channels_conv,\n",
    "                            out_channels = channels_conv,\n",
    "                            kernel_size = kernel_size,\n",
    "                            padding = padding,\n",
    "                            stride = stride,\n",
    "                            dilation = dilation,\n",
    "                            bias = bias,\n",
    "                            batchnorm = batchnorm,\n",
    "                            activation = activation,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            if pooling_type == 'max':\n",
    "                self.convs.append(nn.MaxPool1d(kernel_size=pooling_size, ceil_mode=True))\n",
    "            elif pooling_type == 'avg':\n",
    "                self.convs.append(nn.AvgPool1d(kernel_size=pooling_size, ceil_mode=True))   \n",
    "            elif pooling_type == 'conv':\n",
    "                self.convs.append(\n",
    "                    ConvBlock(\n",
    "                        in_channels = channels_conv,\n",
    "                        out_channels = channels_conv,\n",
    "                        kernel_size = kernel_size,\n",
    "                        padding = padding,\n",
    "                        stride = pooling_size,\n",
    "                        dilation = dilation,\n",
    "                        bias = bias,\n",
    "                        batchnorm = batchnorm,\n",
    "                        activation = activation,\n",
    "                    )\n",
    "                )\n",
    "            elif pooling_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            if pooling_type != 'none':\n",
    "                channels_len = (channels_len + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "                channels_len = (channels_len - pooling_size) // pooling_size + 1\n",
    "                # print(\"channels_len: {}\".format(channels_len))\n",
    "\n",
    "        self.linear_in = nn.Linear(channels_conv * channels_len, channels_linear)\n",
    "        self.linears = nn.ModuleList()\n",
    "        for _ in range(n_layers_linear):\n",
    "            self.linears.append(\n",
    "                Residual(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(channels_linear, channels_linear),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.linear_out = nn.Linear(channels_linear, channels_out)\n",
    "\n",
    "        if loss_type == 'mse':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif loss_type == 'cross_entropy':\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if optimizer_type == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if scheduler_type == 'Cyclic':\n",
    "            self.scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "                self.optimizer, \n",
    "                base_lr = lr / 5,\n",
    "                max_lr = lr * 5,\n",
    "                cycle_momentum = False,\n",
    "            )\n",
    "        elif scheduler_type == 'Plateau':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            print('NO Scheduler')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        self.y_pred = None\n",
    "        self.y_targ = None\n",
    "    \n",
    "    def forward(self, x, z = None):\n",
    "        # print('[0] x.shape:', x.shape)\n",
    "        x = self.conv_in(x)\n",
    "        # print('[1] x.shape:', x.shape)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        # print('[2] x.shape:', x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print('[3] x.shape:', x.shape)\n",
    "\n",
    "        # check if z is Tensor\n",
    "        z_default = torch.zeros(x.shape[0], self.channels_z, device=x.device)\n",
    "        if self.channels_z > 0:\n",
    "            z_default[:, 0] = 1\n",
    "        z = z if isinstance(z, torch.Tensor) else z_default\n",
    "        z = z if z.shape == (x.shape[0], self.channels_z) else z_default\n",
    "\n",
    "        # print('[4] x.shape:', x.shape)\n",
    "        x = self.linear_in(x)\n",
    "        # print('[5] x.shape:', x.shape)\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        # print('[6] x.shape:', x.shape)\n",
    "        x = self.linear_out(x)\n",
    "        # print('[7] x.shape:', x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_targ = batch[:2]\n",
    "        z = batch[2] if len(batch) > 2 and self.channels_z > 0 else None\n",
    "        y_pred = self(x, z=z)\n",
    "        loss = self.criterion(y_pred, y_targ)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_targ = batch[:2]\n",
    "        z = batch[2] if len(batch) > 2 and self.channels_z > 0 else None\n",
    "        y_pred = self(x, z=z)\n",
    "        self.validation_step_outputs.append((y_pred, y_targ))\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        y_pred, y_targ = map(lambda x: torch.cat(x, dim=0), zip(*self.validation_step_outputs))\n",
    "        loss = self.criterion(y_pred, y_targ)\n",
    "        self.log('valid_loss', loss)\n",
    "\n",
    "        for k, name in enumerate(self.names_readout):\n",
    "            corr = torch.corrcoef(torch.stack([y_pred[:, k], y_targ[:, k]]))[0, 1]\n",
    "            self.log('valid_corr_{}'.format(name), corr)\n",
    "        \n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y_targ = batch[:2]\n",
    "        z = batch[2] if len(batch) > 2 and self.channels_z > 0 else None\n",
    "        y_pred = self(x, z=z)\n",
    "        self.test_step_outputs.append((y_pred, y_targ))\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        y_pred, y_targ = map(lambda x: torch.cat(x, dim=0), zip(*self.test_step_outputs))\n",
    "        self.y_pred, self.y_targ = y_pred, y_targ\n",
    "\n",
    "        dict_infer = dict()\n",
    "        loss = self.criterion(y_pred, y_targ)\n",
    "        dict_infer['infer_loss'] = loss\n",
    "\n",
    "        for k, name in enumerate(self.names_readout):\n",
    "            corr = torch.corrcoef(torch.stack([y_pred[:, k], y_targ[:, k]]))[0, 1]\n",
    "            dict_infer['infer_corr_{}'.format(name)] = corr\n",
    "        dict_infer['infer_corr'] = sum([dict_infer['infer_corr_{}'.format(name)] for name in self.names_readout]) / len(self.names_readout)\n",
    "        self.log_dict(dict_infer)\n",
    "        \n",
    "        self.test_step_outputs = []\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = None):\n",
    "        x, y_targ = batch[:2]\n",
    "        z = batch[2] if len(batch) > 2 and self.channels_z > 0 else None\n",
    "        y_pred = self(x, z=z)\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer], [{'scheduler': self.scheduler, 'interval': 'step'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: make those more readable\n",
    "len_seq = mpra_dataset._X.shape[2]\n",
    "names_readout = [name[3:] for name in mpra_dataset.list_Y_names()]\n",
    "\n",
    "module = ResidualCNN(\n",
    "    len_seq = len_seq,\n",
    "    names_readout = names_readout,\n",
    "    channels_out = len(names_readout),\n",
    "    n_layers_conv = 1,\n",
    "    n_convs_per_layer = 5,\n",
    "    n_layers_linear = 2,\n",
    "    pooling_size = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callback_ES = EarlyStopping(monitor='valid_loss', mode='min', patience=5)\n",
    "callback_MC = ModelCheckpoint(monitor='valid_loss', mode='min', save_top_k=1, save_last=True)\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10000, \n",
    "    callbacks=[callback_ES, callback_MC], \n",
    "    accelerator='cuda', \n",
    "    val_check_interval=100, \n",
    "    check_val_every_n_epoch=None,\n",
    "    devices=[2],\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /homes/gws/ylsheng/mpra_test/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== trainer.fit(module, dataloader_train, dataloader_valid) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== trainer.test(module, dataloader_infer) ====\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       infer_corr           0.6433966755867004\n",
      "     infer_corr_expr        0.6433966755867004\n",
      "       infer_loss           0.7691899538040161\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "__SEED__ = 42\n",
    "np.random.seed(__SEED__)\n",
    "torch.manual_seed(__SEED__)\n",
    "\n",
    "print('==== trainer.fit(module, dataloader_train, dataloader_valid) ====')\n",
    "trainer.fit(module, dataloader_train, dataloader_valid)\n",
    "module = module.__class__.load_from_checkpoint(callback_MC.best_model_path)\n",
    "\n",
    "print('==== trainer.test(module, dataloader_infer) ====')\n",
    "results = trainer.test(module, dataloader_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
